{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import scipy.signal\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    Compute  cumulative sums of vectors.\n",
    "\n",
    "    Input: [x0, x1, ..., xn]\n",
    "    Output: [x0 + discount * x1 + discount^2 * x2, x1 + discount * x2, ..., xn]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    \"\"\"Helper function that combines two array shapes.\"\"\"\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    \"\"\"The basic multilayer perceptron architecture used.\"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLPCategoricalActor(nn.Module):\n",
    "    \"\"\"A class for the policy network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        \"\"\"Takes the observation and outputs a distribution over actions.\"\"\"\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        \"\"\"\n",
    "        Take a distribution and action, then gives the log-probability of the action\n",
    "        under that distribution.\n",
    "        \"\"\"\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        \"\"\"\n",
    "        Produce action distributions for given observations, and then compute the\n",
    "        log-likelihood of given actions under those distributions.\n",
    "        \"\"\"\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a\n",
    "\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "    \"\"\"The network used by the value function.\"\"\"\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Critical to ensure v has right shape\n",
    "        return torch.squeeze(self.v_net(obs), -1)\n",
    "\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "    \"\"\"Class to combine policy and value function neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_sizes=(64,64), activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = 8\n",
    "\n",
    "        # Build policy for 4-dimensional action space\n",
    "        self.pi = MLPCategoricalActor(obs_dim, 4, hidden_sizes, activation)\n",
    "\n",
    "        # Build value function\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)\n",
    "\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        Take an state and return action, value function, and log-likelihood\n",
    "        of chosen action.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function.\n",
    "        # It is supposed to return three numbers:\n",
    "        #    1. An action sampled from the policy given a state (0, 1, 2 or 3)\n",
    "        #    2. The value function at the given state\n",
    "        #    3. The log-probability of the action under the policy output distribution\n",
    "        # Hint: This function is only called during inference. You should use\n",
    "        # `torch.no_grad` to ensure that it does not interfer with the gradient computation.\n",
    "        #LP\n",
    "        with torch.no_grad():\n",
    "            Pi, _ = self.pi.forward(state)\n",
    "            val = self.v.forward(state) \n",
    "            act = Pi.sample() \n",
    "            logp_a =  self.pi._log_prob_from_distribution(Pi, act)\n",
    "        return act.item(), val, logp_a\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.step(state)[0]\n",
    "\n",
    "\n",
    "class VPGBuffer:\n",
    "    \"\"\"\n",
    "    Buffer to store trajectories.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma, lam):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        # calculated TD residuals\n",
    "        self.tdres_buf = np.zeros(size, dtype=np.float32)\n",
    "        # rewards\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        # trajectory's remaining return\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        # values predicted\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        # log probabilities of chosen actions under behavior policy\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append a single timestep to the buffer. This is called at each environment\n",
    "        update to store the outcome observed outcome.\n",
    "        \"\"\"\n",
    "        # buffer has to have room so you can store\n",
    "        assert self.ptr < self.max_size\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def end_traj(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call after a trajectory ends. Last value is value(state) if cut-off at a\n",
    "        certain state, or 0 if trajectory ended uninterrupted\n",
    "        \"\"\"\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # TODO: Implement TD residual calculation.\n",
    "        # Hint: we do the discounting for you, you just need to compute 'deltas'.\n",
    "        # see the handout for more info\n",
    "        # deltas = rews[:-1] + ...\n",
    "        #LP\n",
    "        deltas = rews[:-1] + vals[-1] - vals[-2]\n",
    "        self.tdres_buf[path_slice] = discount_cumsum(deltas, self.gamma*self.lam) #using the TD-residual instead of Rt:(Ï„)\n",
    "        #TODO: compute the discounted rewards-to-go. Hint: use the discount_cumsum function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews[:-1], self.gamma*self.lam)\n",
    "\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call after an epoch ends. Resets pointers and returns the buffer contents.\n",
    "        \"\"\"\n",
    "        # Buffer has to be full before you can get something from it.\n",
    "        assert self.ptr == self.max_size\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "\n",
    "        # TODO: Normalize the TD-residuals in self.tdres_buf\n",
    "        #LP\n",
    "        self.tdres_buf =(self.tdres_buf - self.tdres_buf.mean()) / (self.tdres_buf.std() + 1e-12)\n",
    "\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    tdres=self.tdres_buf, logp=self.logp_buf)\n",
    "\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.hid = 64  # layer width of networks\n",
    "        self.l = 2  # layer number of networks\n",
    "        # initialises an actor critic\n",
    "        self.ac = MLPActorCritic(hidden_sizes=[self.hid]*self.l)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "\n",
    "        IMPORTANT: This function called by the checker to train your agent.\n",
    "        You SHOULD NOT change the arguments this function takes and what it outputs!\n",
    "        \"\"\"\n",
    "\n",
    "        # The observations are 8 dimensional vectors, and the actions are numbers,\n",
    "        # i.e. 0-dimensional vectors (hence act_dim is an empty list).\n",
    "        obs_dim = [8]\n",
    "        act_dim = []\n",
    "\n",
    "        # Training parameters\n",
    "        # You may wish to change the following settings for the buffer and training\n",
    "        # Number of training steps per epoch\n",
    "        steps_per_epoch = 3000\n",
    "        # Number of epochs to train for\n",
    "        epochs = 1#50\n",
    "        # The longest an episode can go on before cutting it off\n",
    "        max_ep_len = 300\n",
    "        # Discount factor for weighting future rewards\n",
    "        gamma = 0.99\n",
    "        lam = 0.97\n",
    "\n",
    "        # Learning rates for policy and value function\n",
    "        pi_lr = 3e-3\n",
    "        vf_lr = 1e-3\n",
    "\n",
    "        # Set up buffer\n",
    "        buf = VPGBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
    "\n",
    "        # Initialize the ADAM optimizer using the parameters\n",
    "        # of the policy and then value networks\n",
    "        # TODO: Use these optimizers later to update the policy and value networks.\n",
    "        pi_optimizer = Adam(self.ac.pi.parameters(), lr=pi_lr)\n",
    "        v_optimizer = Adam(self.ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "        # Initialize the environment\n",
    "        state, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "        # Main training loop: collect experience in env and update / log each epoch\n",
    "        for epoch in range(epochs):\n",
    "            ep_returns = []\n",
    "            for t in range(steps_per_epoch):\n",
    "                a, v, logp = self.ac.step(torch.as_tensor(state, dtype=torch.float32))\n",
    "                next_state, r, terminal = self.env.transition(a)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "\n",
    "                # Log transition\n",
    "                buf.store(state, a, r, v, logp)\n",
    "\n",
    "                # Update state (critical!)\n",
    "                state = next_state\n",
    "\n",
    "                timeout = ep_len == max_ep_len\n",
    "                epoch_ended = (t == steps_per_epoch - 1)\n",
    "\n",
    "                if terminal or timeout or epoch_ended:\n",
    "                    # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                    if epoch_ended:\n",
    "                        _, v, _ = self.ac.step(torch.as_tensor(state, dtype=torch.float32))\n",
    "                    else:\n",
    "                        v = 0\n",
    "                    if timeout or terminal:\n",
    "                        ep_returns.append(ep_ret)  # only store return when episode ended\n",
    "                    buf.end_traj(v)\n",
    "                    state, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "            mean_return = np.mean(ep_returns) if len(ep_returns) > 0 else np.nan\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}, mean return {mean_return}\")\n",
    "\n",
    "            # This is the end of an epoch, so here is where you likely want to update\n",
    "            # the policy and / or value function.\n",
    "            # TODO: Implement the polcy and value function update. Hint: some of the torch code is\n",
    "            # done for you.\n",
    "\n",
    "            data = buf.get()\n",
    "            \n",
    "            #Hint: you need to compute a 'loss' such that its derivative with respect to the policy\n",
    "            #parameters is the policy gradient. Then call loss.backwards() and pi_optimizer.step()\n",
    "            #Do 1 policy gradient update\n",
    "            #LP\n",
    "            Pi, _ = self.ac.pi.forward(data['obs'])\n",
    "            act = Pi.sample() \n",
    "            logp_a =  self.ac.pi._log_prob_from_distribution(Pi, act)\n",
    "            ## Option 1 with Rewards to go Rt: \n",
    "            #loss_policy = - 0.5 * ( (gamma **epoch ) * torch.dot(data['ret'], logp_a))**2 \n",
    "            ## Option 3 Rt: with use of a baseline\n",
    "            ## Option 3 with TD residuals, leads to estimators with lower variance\n",
    "            loss_policy = - 0.5 * ( (gamma **epoch ) * torch.dot(data['tdres'], logp_a))**2 \n",
    "            loss_policy.backward()\n",
    "            pi_optimizer.step()\n",
    "            pi_optimizer.zero_grad() #reset the gradient in the policy optimizer\n",
    "            #print(self.ac.pi.logits_net[0].weight.grad)\n",
    "            \n",
    "\n",
    "            \n",
    "            #We suggest to do 100 iterations of value function updates\n",
    "            #compute a loss for the value function, call loss.backwards() and then #v_optimizer.step()\n",
    "            #LP\n",
    "            # or do we want to accumulate gradients? No\n",
    "            for _ in range(100):\n",
    "                Val = self.ac.v.forward(data['obs'])\n",
    "                #When training the value function, the reward-to-go can be used as a target for the loss.\n",
    "                criterion = nn.MSELoss()\n",
    "                loss_valfn = criterion(Val, data['ret'] )\n",
    "                v_optimizer.zero_grad()\n",
    "                loss_valfn.backward()#retain_graph=True)#retain_graph=True)\n",
    "                v_optimizer.step()\n",
    "                #print(self.ac.v.v_net[0].weight.grad)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\"\n",
    "        Sample an action from your policy.\n",
    "\n",
    "        IMPORTANT: This function called by the checker to evaluate your agent.\n",
    "        You SHOULD NOT change the arguments this function takes and what it outputs!\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function.\n",
    "        # Currently, this just returns a random action.\n",
    "        #LP\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        return self.ac.act(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9415, 0.1167]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.6294, -0.3341]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "model = nn.Linear(2, 2)\n",
    "x = torch.randn(1, 2)\n",
    "target = torch.randn(1, 2)\n",
    "output = model(x)\n",
    "print(output)\n",
    "print(target)\n",
    "loss = my_loss(output, target)\n",
    "\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, mean return -344.7250352729035\n",
      "Evaluating agent...\n",
      "Testing policy: episode 1/100\n",
      "Achieved -27.72 return.\n",
      "Testing policy: episode 2/100\n",
      "Achieved -33.86 return.\n",
      "Testing policy: episode 3/100\n",
      "Achieved -53.79 return.\n",
      "Testing policy: episode 4/100\n",
      "Achieved -32.58 return.\n",
      "Testing policy: episode 5/100\n",
      "Achieved -11.44 return.\n",
      "Testing policy: episode 6/100\n",
      "Achieved 5.28 return.\n",
      "Testing policy: episode 7/100\n",
      "Achieved -41.90 return.\n",
      "Testing policy: episode 8/100\n",
      "Achieved -35.08 return.\n",
      "Testing policy: episode 9/100\n",
      "Achieved -39.99 return.\n",
      "Testing policy: episode 10/100\n",
      "Achieved -22.80 return.\n",
      "Testing policy: episode 11/100\n",
      "Achieved -32.52 return.\n",
      "Saved video of 10 episodes to 'policy.mp4'.\n",
      "Testing policy: episode 12/100\n",
      "Achieved 7.77 return.\n",
      "Testing policy: episode 13/100\n",
      "Achieved -24.85 return.\n",
      "Testing policy: episode 14/100\n",
      "Achieved -24.97 return.\n",
      "Testing policy: episode 15/100\n",
      "Achieved -26.93 return.\n",
      "Testing policy: episode 16/100\n",
      "Achieved -51.42 return.\n",
      "Testing policy: episode 17/100\n",
      "Achieved -42.15 return.\n",
      "Testing policy: episode 18/100\n",
      "Achieved -67.47 return.\n",
      "Testing policy: episode 19/100\n",
      "Achieved -41.09 return.\n",
      "Testing policy: episode 20/100\n",
      "Achieved -12.83 return.\n",
      "Testing policy: episode 21/100\n",
      "Achieved -30.30 return.\n",
      "Testing policy: episode 22/100\n",
      "Achieved -20.00 return.\n",
      "Testing policy: episode 23/100\n",
      "Achieved -39.15 return.\n",
      "Testing policy: episode 24/100\n",
      "Achieved -28.76 return.\n",
      "Testing policy: episode 25/100\n",
      "Achieved -9.95 return.\n",
      "Testing policy: episode 26/100\n",
      "Achieved -26.06 return.\n",
      "Testing policy: episode 27/100\n",
      "Achieved -83.22 return.\n",
      "Testing policy: episode 28/100\n",
      "Achieved -0.37 return.\n",
      "Testing policy: episode 29/100\n",
      "Achieved -73.88 return.\n",
      "Testing policy: episode 30/100\n",
      "Achieved -4.09 return.\n",
      "Testing policy: episode 31/100\n",
      "Achieved -26.80 return.\n",
      "Testing policy: episode 32/100\n",
      "Achieved -25.37 return.\n",
      "Testing policy: episode 33/100\n",
      "Achieved -48.67 return.\n",
      "Testing policy: episode 34/100\n",
      "Achieved -2.04 return.\n",
      "Testing policy: episode 35/100\n",
      "Achieved -73.99 return.\n",
      "Testing policy: episode 36/100\n",
      "Achieved -25.27 return.\n",
      "Testing policy: episode 37/100\n",
      "Achieved -17.32 return.\n",
      "Testing policy: episode 38/100\n",
      "Achieved -46.30 return.\n",
      "Testing policy: episode 39/100\n",
      "Achieved -70.67 return.\n",
      "Testing policy: episode 40/100\n",
      "Achieved -38.49 return.\n",
      "Testing policy: episode 41/100\n",
      "Achieved -11.57 return.\n",
      "Testing policy: episode 42/100\n",
      "Achieved -20.44 return.\n",
      "Testing policy: episode 43/100\n",
      "Achieved 3.95 return.\n",
      "Testing policy: episode 44/100\n",
      "Achieved 4.53 return.\n",
      "Testing policy: episode 45/100\n",
      "Achieved -9.82 return.\n",
      "Testing policy: episode 46/100\n",
      "Achieved -12.21 return.\n",
      "Testing policy: episode 47/100\n",
      "Achieved -39.68 return.\n",
      "Testing policy: episode 48/100\n",
      "Achieved -31.07 return.\n",
      "Testing policy: episode 49/100\n",
      "Achieved -21.14 return.\n",
      "Testing policy: episode 50/100\n",
      "Achieved 0.76 return.\n",
      "Testing policy: episode 51/100\n",
      "Achieved -55.50 return.\n",
      "Testing policy: episode 52/100\n",
      "Achieved -55.91 return.\n",
      "Testing policy: episode 53/100\n",
      "Achieved -46.26 return.\n",
      "Testing policy: episode 54/100\n",
      "Achieved 10.18 return.\n",
      "Testing policy: episode 55/100\n",
      "Achieved -41.27 return.\n",
      "Testing policy: episode 56/100\n",
      "Achieved -40.32 return.\n",
      "Testing policy: episode 57/100\n",
      "Achieved -34.15 return.\n",
      "Testing policy: episode 58/100\n",
      "Achieved -26.70 return.\n",
      "Testing policy: episode 59/100\n",
      "Achieved -26.60 return.\n",
      "Testing policy: episode 60/100\n",
      "Achieved -70.25 return.\n",
      "Testing policy: episode 61/100\n",
      "Achieved -32.71 return.\n",
      "Testing policy: episode 62/100\n",
      "Achieved -2.25 return.\n",
      "Testing policy: episode 63/100\n",
      "Achieved -28.86 return.\n",
      "Testing policy: episode 64/100\n",
      "Achieved 16.27 return.\n",
      "Testing policy: episode 65/100\n",
      "Achieved 4.58 return.\n",
      "Testing policy: episode 66/100\n",
      "Achieved -11.24 return.\n",
      "Testing policy: episode 67/100\n",
      "Achieved -42.80 return.\n",
      "Testing policy: episode 68/100\n",
      "Achieved -6.43 return.\n",
      "Testing policy: episode 69/100\n",
      "Achieved -4.75 return.\n",
      "Testing policy: episode 70/100\n",
      "Achieved -41.17 return.\n",
      "Testing policy: episode 71/100\n",
      "Achieved -5.77 return.\n",
      "Testing policy: episode 72/100\n",
      "Achieved -51.80 return.\n",
      "Testing policy: episode 73/100\n",
      "Achieved -36.06 return.\n",
      "Testing policy: episode 74/100\n",
      "Achieved -76.72 return.\n",
      "Testing policy: episode 75/100\n",
      "Achieved -24.00 return.\n",
      "Testing policy: episode 76/100\n",
      "Achieved -55.76 return.\n",
      "Testing policy: episode 77/100\n",
      "Achieved -14.20 return.\n",
      "Testing policy: episode 78/100\n",
      "Achieved -88.34 return.\n",
      "Testing policy: episode 79/100\n",
      "Achieved -44.28 return.\n",
      "Testing policy: episode 80/100\n",
      "Achieved -4.18 return.\n",
      "Testing policy: episode 81/100\n",
      "Achieved -13.86 return.\n",
      "Testing policy: episode 82/100\n",
      "Achieved -26.46 return.\n",
      "Testing policy: episode 83/100\n",
      "Achieved -81.25 return.\n",
      "Testing policy: episode 84/100\n",
      "Achieved 17.99 return.\n",
      "Testing policy: episode 85/100\n",
      "Achieved -27.15 return.\n",
      "Testing policy: episode 86/100\n",
      "Achieved -41.14 return.\n",
      "Testing policy: episode 87/100\n",
      "Achieved -42.18 return.\n",
      "Testing policy: episode 88/100\n",
      "Achieved 28.22 return.\n",
      "Testing policy: episode 89/100\n",
      "Achieved -41.42 return.\n",
      "Testing policy: episode 90/100\n",
      "Achieved -60.24 return.\n",
      "Testing policy: episode 91/100\n",
      "Achieved -27.80 return.\n",
      "Testing policy: episode 92/100\n",
      "Achieved -13.52 return.\n",
      "Testing policy: episode 93/100\n",
      "Achieved -39.04 return.\n",
      "Testing policy: episode 94/100\n",
      "Achieved -57.89 return.\n",
      "Testing policy: episode 95/100\n",
      "Achieved -33.08 return.\n",
      "Testing policy: episode 96/100\n",
      "Achieved 0.79 return.\n",
      "Testing policy: episode 97/100\n",
      "Achieved -52.98 return.\n",
      "Testing policy: episode 98/100\n",
      "Achieved -56.92 return.\n",
      "Testing policy: episode 99/100\n",
      "Achieved -35.57 return.\n",
      "Testing policy: episode 100/100\n",
      "Achieved -30.13 return.\n",
      "Average return: -30.09\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Train and evaluate agent.\n",
    "\n",
    "    This function basically does the same as the checker that evaluates your agent.\n",
    "    You can use it for debugging your agent and visualizing what it does.\n",
    "    \"\"\"\n",
    "    from lunar_lander import LunarLander\n",
    "    from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "    env = LunarLander()\n",
    "\n",
    "    agent = Agent(env)\n",
    "    agent.train()\n",
    "\n",
    "    rec = VideoRecorder(env, \"policy.mp4\")\n",
    "    episode_length = 10\n",
    "    n_eval = 100\n",
    "    returns = []\n",
    "    print(\"Evaluating agent...\")\n",
    "\n",
    "    for i in range(n_eval):\n",
    "        print(f\"Testing policy: episode {i+1}/{n_eval}\")\n",
    "        state = env.reset()\n",
    "        cumulative_return = 0\n",
    "        # The environment will set terminal to True if an episode is done.\n",
    "        terminal = False\n",
    "        env.reset()\n",
    "        for t in range(episode_length):\n",
    "#             if i <= 10:\n",
    "#                 rec.capture_frame() #ONLY FOR EVALUATION \n",
    "#             # Taking an action in the environment\n",
    "            action = agent.get_action(state)\n",
    "            state, reward, terminal = env.transition(action)\n",
    "            cumulative_return += reward\n",
    "            if terminal:\n",
    "                break\n",
    "        returns.append(cumulative_return)\n",
    "        print(f\"Achieved {cumulative_return:.2f} return.\")\n",
    "        if i == 10:\n",
    "            rec.close()\n",
    "            print(\"Saved video of 10 episodes to 'policy.mp4'.\")\n",
    "    env.close()\n",
    "    print(f\"Average return: {np.mean(returns):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai_2020",
   "language": "python",
   "name": "pai_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
