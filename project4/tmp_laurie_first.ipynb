{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import scipy.signal\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    Compute  cumulative sums of vectors.\n",
    "\n",
    "    Input: [x0, x1, ..., xn]\n",
    "    Output: [x0 + discount * x1 + discount^2 * x2, x1 + discount * x2, ..., xn]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    \"\"\"Helper function that combines two array shapes.\"\"\"\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    \"\"\"The basic multilayer perceptron architecture used.\"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLPCategoricalActor(nn.Module):\n",
    "    \"\"\"A class for the policy network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        \"\"\"Takes the observation and outputs a distribution over actions.\"\"\"\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        \"\"\"\n",
    "        Take a distribution and action, then gives the log-probability of the action\n",
    "        under that distribution.\n",
    "        \"\"\"\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        \"\"\"\n",
    "        Produce action distributions for given observations, and then compute the\n",
    "        log-likelihood of given actions under those distributions.\n",
    "        \"\"\"\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a\n",
    "\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "    \"\"\"The network used by the value function.\"\"\"\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Critical to ensure v has right shape\n",
    "        return torch.squeeze(self.v_net(obs), -1)\n",
    "\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "    \"\"\"Class to combine policy and value function neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_sizes=(64,64), activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = 8\n",
    "\n",
    "        # Build policy for 4-dimensional action space\n",
    "        self.pi = MLPCategoricalActor(obs_dim, 4, hidden_sizes, activation)\n",
    "\n",
    "        # Build value function\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)\n",
    "\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        Take an state and return action, value function, and log-likelihood\n",
    "        of chosen action.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function.\n",
    "        # It is supposed to return three numbers:\n",
    "        #    1. An action sampled from the policy given a state (0, 1, 2 or 3)\n",
    "        #    2. The value function at the given state\n",
    "        #    3. The log-probability of the action under the policy output distribution\n",
    "        # Hint: This function is only called during inference. You should use\n",
    "        # `torch.no_grad` to ensure that it does not interfer with the gradient computation.\n",
    "        #LP\n",
    "        with torch.no_grad():\n",
    "            pi, logp_a = self.pi.forward(state)\n",
    "            val = self.v.forward(state) \n",
    "            act = pi.sample() \n",
    "        return act.item(), val, logp_a\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.step(state)[0]\n",
    "\n",
    "\n",
    "class VPGBuffer:\n",
    "    \"\"\"\n",
    "    Buffer to store trajectories.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma, lam):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        # calculated TD residuals\n",
    "        self.tdres_buf = np.zeros(size, dtype=np.float32)\n",
    "        # rewards\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        # trajectory's remaining return\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        # values predicted\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        # log probabilities of chosen actions under behavior policy\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append a single timestep to the buffer. This is called at each environment\n",
    "        update to store the outcome observed outcome.\n",
    "        \"\"\"\n",
    "        # buffer has to have room so you can store\n",
    "        assert self.ptr < self.max_size\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def end_traj(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call after a trajectory ends. Last value is value(state) if cut-off at a\n",
    "        certain state, or 0 if trajectory ended uninterrupted\n",
    "        \"\"\"\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # TODO: Implement TD residual calculation.\n",
    "        # Hint: we do the discounting for you, you just need to compute 'deltas'.\n",
    "        # see the handout for more info\n",
    "        # deltas = rews[:-1] + ...\n",
    "        #LP\n",
    "        deltas = rews[:-1] + vals[-1] - vals[-2]\n",
    "        self.tdres_buf[path_slice] = discount_cumsum(deltas, self.gamma*self.lam) #using the TD-residual instead of Rt:(Ï„)\n",
    "        #TODO: compute the discounted rewards-to-go. Hint: use the discount_cumsum function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews[:-1], self.gamma*self.lam)\n",
    "\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call after an epoch ends. Resets pointers and returns the buffer contents.\n",
    "        \"\"\"\n",
    "        # Buffer has to be full before you can get something from it.\n",
    "        assert self.ptr == self.max_size\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "\n",
    "        # TODO: Normalize the TD-residuals in self.tdres_buf\n",
    "        #LP\n",
    "        self.tdres_buf =(self.tdres_buf - self.tdres_buf.mean()) / (self.tdres_buf.std() + 1e-12)\n",
    "\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    tdres=self.tdres_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.hid = 64  # layer width of networks\n",
    "        self.l = 2  # layer number of networks\n",
    "        # initialises an actor critic\n",
    "        self.ac = MLPActorCritic(hidden_sizes=[self.hid]*self.l)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "\n",
    "        IMPORTANT: This function called by the checker to train your agent.\n",
    "        You SHOULD NOT change the arguments this function takes and what it outputs!\n",
    "        \"\"\"\n",
    "\n",
    "        # The observations are 8 dimensional vectors, and the actions are numbers,\n",
    "        # i.e. 0-dimensional vectors (hence act_dim is an empty list).\n",
    "        obs_dim = [8]\n",
    "        act_dim = []\n",
    "\n",
    "        # Training parameters\n",
    "        # You may wish to change the following settings for the buffer and training\n",
    "        # Number of training steps per epoch\n",
    "        steps_per_epoch = 3000\n",
    "        # Number of epochs to train for\n",
    "        epochs = 50\n",
    "        # The longest an episode can go on before cutting it off\n",
    "        max_ep_len = 300\n",
    "        # Discount factor for weighting future rewards\n",
    "        gamma = 0.99\n",
    "        lam = 0.97\n",
    "\n",
    "        # Learning rates for policy and value function\n",
    "        pi_lr = 3e-3\n",
    "        vf_lr = 1e-3\n",
    "\n",
    "        # Set up buffer\n",
    "        buf = VPGBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
    "\n",
    "        # Initialize the ADAM optimizer using the parameters\n",
    "        # of the policy and then value networks\n",
    "        # TODO: Use these optimizers later to update the policy and value networks.\n",
    "        pi_optimizer = Adam(self.ac.pi.parameters(), lr=pi_lr)\n",
    "        v_optimizer = Adam(self.ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "        # Initialize the environment\n",
    "        state, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "        # Main training loop: collect experience in env and update / log each epoch\n",
    "        for epoch in range(epochs):\n",
    "            ep_returns = []\n",
    "            for t in range(steps_per_epoch):\n",
    "                a, v, logp = self.ac.step(torch.as_tensor(state, dtype=torch.float32))\n",
    "\n",
    "                next_state, r, terminal = self.env.transition(a)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "\n",
    "                # Log transition\n",
    "                buf.store(state, a, r, v, logp)\n",
    "\n",
    "                # Update state (critical!)\n",
    "                state = next_state\n",
    "\n",
    "                timeout = ep_len == max_ep_len\n",
    "                epoch_ended = (t == steps_per_epoch - 1)\n",
    "\n",
    "                if terminal or timeout or epoch_ended:\n",
    "                    # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                    if epoch_ended:\n",
    "                        _, v, _ = self.ac.step(torch.as_tensor(state, dtype=torch.float32))\n",
    "                    else:\n",
    "                        v = 0\n",
    "                    if timeout or terminal:\n",
    "                        ep_returns.append(ep_ret)  # only store return when episode ended\n",
    "                    buf.end_traj(v)\n",
    "                    state, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "            mean_return = np.mean(ep_returns) if len(ep_returns) > 0 else np.nan\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}, mean return {mean_return}\")\n",
    "\n",
    "            # This is the end of an epoch, so here is where you likely want to update\n",
    "            # the policy and / or value function.\n",
    "            # TODO: Implement the polcy and value function update. Hint: some of the torch code is\n",
    "            # done for you.\n",
    "            \n",
    "            data = buf.get()\n",
    "\n",
    "            #Do 1 policy gradient update\n",
    "            pi_optimizer.zero_grad() #reset the gradient in the policy optimizer\n",
    "\n",
    "            #Hint: you need to compute a 'loss' such that its derivative with respect to the policy\n",
    "            #parameters is the policy gradient. Then call loss.backwards() and pi_optimizer.step()\n",
    "\n",
    "            #We suggest to do 100 iterations of value function updates\n",
    "            for _ in range(100):\n",
    "                v_optimizer.zero_grad()\n",
    "                #compute a loss for the value function, call loss.backwards() and then\n",
    "                #v_optimizer.step()\n",
    "\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\"\n",
    "        Sample an action from your policy.\n",
    "\n",
    "        IMPORTANT: This function called by the checker to evaluate your agent.\n",
    "        You SHOULD NOT change the arguments this function takes and what it outputs!\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function.\n",
    "        # Currently, this just returns a random action.\n",
    "        return np.random.choice([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, mean return -335.76055014220043\n",
      "Epoch: 2/50, mean return -354.75012802108876\n",
      "Epoch: 3/50, mean return -342.1658435944827\n",
      "Epoch: 4/50, mean return -381.626195701039\n",
      "Epoch: 5/50, mean return -350.63728509583706\n",
      "Epoch: 6/50, mean return -348.1568573261207\n",
      "Epoch: 7/50, mean return -325.5364157030278\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-8d781f40e2b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-8d781f40e2b2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoRecorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"policy.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-7af035334f69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-7af035334f69>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m#LP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda/envs/pai_2020/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Train and evaluate agent.\n",
    "\n",
    "    This function basically does the same as the checker that evaluates your agent.\n",
    "    You can use it for debugging your agent and visualizing what it does.\n",
    "    \"\"\"\n",
    "    from lunar_lander import LunarLander\n",
    "    from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "    env = LunarLander()\n",
    "\n",
    "    agent = Agent(env)\n",
    "    agent.train()\n",
    "\n",
    "    rec = VideoRecorder(env, \"policy.mp4\")\n",
    "    episode_length = 300\n",
    "    n_eval = 100\n",
    "    returns = []\n",
    "    print(\"Evaluating agent...\")\n",
    "\n",
    "    for i in range(n_eval):\n",
    "        print(f\"Testing policy: episode {i+1}/{n_eval}\")\n",
    "        state = env.reset()\n",
    "        cumulative_return = 0\n",
    "        # The environment will set terminal to True if an episode is done.\n",
    "        terminal = False\n",
    "        env.reset()\n",
    "        for t in range(episode_length):\n",
    "            if i <= 10:\n",
    "                rec.capture_frame()\n",
    "            # Taking an action in the environment\n",
    "            action = agent.get_action(state)\n",
    "            state, reward, terminal = env.transition(action)\n",
    "            cumulative_return += reward\n",
    "            if terminal:\n",
    "                break\n",
    "        returns.append(cumulative_return)\n",
    "        print(f\"Achieved {cumulative_return:.2f} return.\")\n",
    "        if i == 10:\n",
    "            rec.close()\n",
    "            print(\"Saved video of 10 episodes to 'policy.mp4'.\")\n",
    "    env.close()\n",
    "    print(f\"Average return: {np.mean(returns):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai_2020",
   "language": "python",
   "name": "pai_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
