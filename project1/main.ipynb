{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as datetime\n",
    "\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, Matern, RBF\n",
    "\n",
    "import kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import solution\n",
    "from solution import Model\n",
    "from solution import cv_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy code to override default classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main():\n",
    "train_x_name = \"train_x.csv\"\n",
    "train_y_name = \"train_y.csv\"\n",
    "\n",
    "train_x = np.loadtxt(train_x_name, delimiter=',')\n",
    "train_y = np.loadtxt(train_y_name, delimiter=',')\n",
    "\n",
    "# load the test dateset\n",
    "test_x_name = \"test_x.csv\"\n",
    "test_x = np.loadtxt(test_x_name, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose what data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_data_frac = 0.01\n",
    "use_right = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1710, 3)\n"
     ]
    }
   ],
   "source": [
    "df_vals = np.stack([train_x[:,0],train_x[:,1],train_y],axis=1)\n",
    "df = pd.DataFrame(data = df_vals,columns = ['x0','x1','y'])\n",
    "df_left = df[df['x0']<-0.5]\n",
    "df_left = df_left.sample(frac=left_data_frac,random_state=42)\n",
    "\n",
    "if use_right:\n",
    "    df_right = df[df['x0']>-0.5]\n",
    "    df_left_right = pd.concat([df_left,df_right])\n",
    "    train_x = df_left_right[['x0','x1']].values\n",
    "    train_y = df_left_right['y'].values\n",
    "    print(df_left_right.shape)\n",
    "else:\n",
    "    train_x = df_left[['x0','x1']].values\n",
    "    train_y = df_left['y'].values\n",
    "    print(df_left.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1710, 2)\n"
     ]
    }
   ],
   "source": [
    "def tune_HP_nested_CV(kernels_grid, K_cv, model_config):\n",
    "    train_x_split, test_x_split, train_y_split, test_y_split = train_test_split(train_x, train_y, test_size=0.33, random_state=42)\n",
    " \n",
    "    CV_res = []\n",
    "    for hp_kernel in kernels_grid:\n",
    "        model_config[\"kernel\"] = hp_kernel\n",
    "        cv_m = cv_eval(cv_splits = K_cv,\n",
    "               cv_preprocess_left_frac = 0.05,\n",
    "               model_config=model_config)\n",
    "        val_cost_array = cv_m.run_cross_validation(train_x_split, train_y_split)\n",
    "        print(np.mean(val_cost_array))\n",
    "        print(np.std(val_cost_array))\n",
    "        CV_res.append(np.mean(val_cost_array))\n",
    "    return CV_res\n",
    "\n",
    "print(np.shape(train_x))\n",
    "#nested n1*k*n2 total\n",
    "def tune_HP_nested_CV2(kernels_grid, K_cv, model_config,result_csv_path):\n",
    "    cv_result_cols = [\"test_cost\",\"val_cost\",\"val_lml\",\"init_kernel\",\n",
    "                              \"left_data\",\"cv_config\",\"model_config\",\"time\"]\n",
    "    \n",
    "    n_outer = 2\n",
    "    n_kernels = len(kernels_grid)\n",
    "    n_inner = K_cv\n",
    "    cv_config = {\"outer\":n_outer,\"kernels\":n_kernels,\"inner\":n_inner}\n",
    "    \n",
    "    cv_outer = KFold(n_splits=n_outer, shuffle=True, random_state=42)\n",
    "    outer_res = []\n",
    "    metrics_res = np.zeros((3,n_outer,n_kernels))\n",
    "\n",
    "    exp_start_time = datetime.datetime.now()\n",
    "    \n",
    "    outer_count = 0\n",
    "    for train_ix,test_ix in cv_outer.split(train_x):\n",
    "        train_x_split,test_x_split = train_x[train_ix,:],train_x[test_ix,:]\n",
    "        train_y_split,test_y_split = train_y[train_ix],train_y[test_ix]\n",
    "        #train_x_split, test_x_split, train_y_split, test_y_split = train_test_split(train_x, train_y, test_size=0.33, random_state=42)\n",
    " \n",
    "        kernel_count = 0\n",
    "        for hp_kernel in kernels_grid:\n",
    "            print(\"outer=%d\"%outer_count)\n",
    "            print(\"kernel=\"+str(hp_kernel))\n",
    "            model_config[\"kernel\"] = hp_kernel\n",
    "            cv_m = cv_eval(cv_splits = K_cv,\n",
    "                   cv_preprocess_left_frac = 1.0,\n",
    "                   model_config=model_config)\n",
    "            val_cost_array,lml_array = cv_m.run_cross_validation(train_x_split, train_y_split)\n",
    "\n",
    "            #inner_res = np.append(inner_res,np.mean(val_cost_array))\n",
    "            metrics_res[1][outer_count][kernel_count] = np.mean(val_cost_array)\n",
    "            metrics_res[2][outer_count][kernel_count] = np.mean(lml_array)\n",
    "            \n",
    "            test_model = solution.Model(model_config)\n",
    "            test_model.fit_model(train_x_split,train_y_split)\n",
    "            test_y_pred = test_model.predict(test_x_split)\n",
    "            test_cost = solution.cost_function(test_y_split,test_y_pred)\n",
    "            metrics_res[0][outer_count][kernel_count] = test_cost\n",
    "            \n",
    "            print(test_cost)\n",
    "            print(np.mean(val_cost_array))\n",
    "            print(np.std(val_cost_array))\n",
    "            \"\"\"\n",
    "            df_cv_result = pd.DataFrame(columns = cv_result_cols)\n",
    "            #df_cv_result = pd.read_csv(result_csv_path)\n",
    "            \n",
    "            df_cv_result = df_cv_result.append({\n",
    "                          \"test_cost\":test_cost,\n",
    "                          \"val_cost\":np.mean(val_cost_array),\n",
    "                          \"val_lml\":np.mean(lml_array),\n",
    "                          \"init_kernel\":str(hp_kernel),\n",
    "                          \"left_data\":int(df_left.shape[0]),\n",
    "                          \"cv_config\":str(cv_config),\n",
    "                          \"model_config\":str(model_config),\n",
    "                          \"time\":datetime.datetime.now()\n",
    "                         },ignore_index=True)\n",
    "            df_cv_result.to_csv(result_csv_path,index=False,header=False,mode=\"a\")\n",
    "            \"\"\"\n",
    "            kernel_count+=1            \n",
    "        outer_count += 1\n",
    "        \n",
    "    for i in range(n_kernels):\n",
    "        df_cv_result = pd.DataFrame(columns = cv_result_cols)\n",
    "        df_cv_result = df_cv_result.append({\n",
    "                      \"test_cost\":np.mean(metrics_res[0,:,i]),\n",
    "                      \"val_cost\":np.mean(metrics_res[1,:,i]),\n",
    "                      \"val_lml\":np.mean(metrics_res[2,:,i]),\n",
    "                      \"init_kernel\":str(kernels_grid[i]),\n",
    "                      \"left_data\":int(df_left.shape[0]),\n",
    "                      \"cv_config\":str(cv_config),\n",
    "                      \"model_config\":str(model_config),\n",
    "                      \"time\":exp_start_time\n",
    "                     },ignore_index=True)\n",
    "        df_cv_result.to_csv(result_csv_path,index=False,header=False,mode=\"a\")\n",
    "\n",
    "    #return {\"outer\":outer_res,\"inner\":inner_res}\n",
    "    return metrics_res\n",
    "\n",
    "def tune_HP_nested_LML(kernel_tunable, K_cv, model_config,result_csv_path):\n",
    "    n_outer = 5\n",
    "    n_inner = K_cv\n",
    "    cv_config = {\"outer\":n_outer,\"kernels\":None,\"inner\":n_inner}\n",
    "    \n",
    "    # automatic Hyperparameters optimisation with scikit-learn \n",
    "    model_config[\"use_skit_learn\"] = True\n",
    "    model_config[\"use_nystrom\"] = False\n",
    "    model_config[\"use_fitc\"] = False\n",
    "    model_config[\"kernel\"] = kernel_tunable\n",
    "    grp = Model(model_config)\n",
    "    \n",
    "    cv_outer = KFold(n_splits=n_outer, shuffle=True, random_state=42)\n",
    "    outer_res = []\n",
    "    lml_array = np.array([])\n",
    "    \n",
    "    outer_count = 0\n",
    "    for train_ix,test_ix in cv_outer.split(train_x):\n",
    "        train_x_split,test_x_split = train_x[train_ix,:],train_x[test_ix,:]\n",
    "        train_y_split,test_y_split = train_y[train_ix],train_y[test_ix]\n",
    "        #train_x_split, test_x_split, train_y_split, test_y_split = train_test_split(train_x, train_y, test_size=0.33, random_state=42)\n",
    " \n",
    "     \n",
    "        cv_result_cols = [\"test_cost\",\"val_cost\",\"val_lml\",\"init_kernel\",\n",
    "                          \"left_data\",\"cv_config\",\"model_config\",\"time\"]\n",
    "        df_cv_result = pd.DataFrame(columns = cv_result_cols)\n",
    "        #df_cv_result = pd.read_csv(result_csv_path)\n",
    "        print(\"outer=%d\"%outer_count)\n",
    "\n",
    "        # Fit model on test set \n",
    "        grp.fit_model(train_x = train_x_split, train_y = train_y_split)\n",
    "        val_llikelihood = grp.likelihood()\n",
    "        print(\"val log likelihood fn        %f\"%(val_llikelihood))\n",
    "        lml_array = np.append(lml_array,val_llikelihood)\n",
    "        #print(fitted.kernel_.get_params())\n",
    "\n",
    "        # Evaluate model on test set \n",
    "        test_y_pred = fitted.predict(test_x_split)\n",
    "        test_cost = solution.cost_function(test_y_split,test_y_pred)\n",
    "        outer_res.append(test_cost)\n",
    "        print(\"cost on test set{}\".format(test_cost))\n",
    "\n",
    "        df_cv_result = df_cv_result.append({\n",
    "                      \"test_cost\":test_cost,\n",
    "                      \"val_cost\": None,\n",
    "                      \"val_lml\":np.mean(lml_array),\n",
    "                      \"init_kernel\":str(kernel_tunable),\n",
    "                      \"left_data\":int(df_left.shape[0]),\n",
    "                      \"cv_config\":str(cv_config),\n",
    "                      \"model_config\":str(model_config),\n",
    "                      \"time\":datetime.datetime.now()\n",
    "                     },ignore_index=True)\n",
    "        df_cv_result.to_csv(result_csv_path,index=False,header=False,mode=\"a\")\n",
    "       \n",
    "        outer_count += 1\n",
    "    print(\"Mean cost on test set{}\".format(np.mean(outer_res)))\n",
    "    return {\"outer\":outer_res}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Call Scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer=0\n",
      "kernel=RBF(length_scale=1)\n",
      "(1368, 3)\n",
      "i=0\n",
      "logging_setup\n",
      "training cost fn   0.051183\n",
      "val cost fn        7.114097\n",
      "val log likelihood fn        -319859718.017057\n",
      "\n",
      "\n",
      "i=1\n",
      "logging_setup\n",
      "training cost fn   0.051183\n",
      "val cost fn        7.114097\n",
      "val log likelihood fn        -319859718.017057\n",
      "\n",
      "\n",
      "i=2\n",
      "logging_setup\n",
      "training cost fn   0.051183\n",
      "val cost fn        7.114097\n",
      "val log likelihood fn        -319859718.017057\n",
      "\n",
      "\n",
      "[-3.19859718e+08 -3.19859718e+08 -3.19859718e+08]\n",
      "logging_setup\n",
      "4.7014748839108\n",
      "7.1140974392303065\n",
      "0.0\n",
      "outer=0\n",
      "kernel=1**2 * RBF(length_scale=1) + WhiteKernel(noise_level=1)\n",
      "(1368, 3)\n",
      "i=0\n",
      "logging_setup\n",
      "training cost fn   0.054964\n",
      "val cost fn        0.054739\n",
      "val log likelihood fn        1282.128277\n",
      "\n",
      "\n",
      "i=1\n",
      "logging_setup\n",
      "training cost fn   0.054964\n",
      "val cost fn        0.054739\n",
      "val log likelihood fn        1282.128277\n",
      "\n",
      "\n",
      "i=2\n",
      "logging_setup\n",
      "training cost fn   0.054964\n",
      "val cost fn        0.054739\n",
      "val log likelihood fn        1282.128277\n",
      "\n",
      "\n",
      "[1282.12827689 1282.12827689 1282.12827689]\n",
      "logging_setup\n",
      "0.05578165244473191\n",
      "0.05473901866362233\n",
      "6.938893903907228e-18\n",
      "outer=0\n",
      "kernel=1**2 * Matern(length_scale=1, nu=1.5) + WhiteKernel(noise_level=1)\n",
      "(1368, 3)\n",
      "i=0\n",
      "logging_setup\n",
      "training cost fn   0.054359\n",
      "val cost fn        0.054342\n",
      "val log likelihood fn        1315.554732\n",
      "\n",
      "\n",
      "i=1\n",
      "logging_setup\n",
      "training cost fn   0.054359\n",
      "val cost fn        0.054342\n",
      "val log likelihood fn        1315.554732\n",
      "\n",
      "\n",
      "i=2\n",
      "logging_setup\n",
      "training cost fn   0.054359\n",
      "val cost fn        0.054342\n",
      "val log likelihood fn        1315.554732\n",
      "\n",
      "\n",
      "[1315.55473209 1315.55473209 1315.55473209]\n",
      "logging_setup\n",
      "0.05574408688916637\n",
      "0.05434151888649743\n",
      "6.938893903907228e-18\n",
      "outer=0\n",
      "kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1)\n",
      "(1368, 3)\n",
      "i=0\n",
      "logging_setup\n",
      "training cost fn   0.054440\n",
      "val cost fn        0.054348\n",
      "val log likelihood fn        1305.068179\n",
      "\n",
      "\n",
      "i=1\n",
      "logging_setup\n",
      "training cost fn   0.054440\n",
      "val cost fn        0.054348\n",
      "val log likelihood fn        1305.068179\n",
      "\n",
      "\n",
      "i=2\n",
      "logging_setup\n",
      "training cost fn   0.054440\n",
      "val cost fn        0.054348\n",
      "val log likelihood fn        1305.068179\n",
      "\n",
      "\n",
      "[1305.0681791 1305.0681791 1305.0681791]\n",
      "logging_setup\n",
      "0.05569156134406133\n",
      "0.05434832890340652\n",
      "0.0\n",
      "outer=0\n",
      "kernel=1**2 + 1**2 * Matern(length_scale=1, nu=1.5) + WhiteKernel(noise_level=1)\n",
      "(1368, 3)\n",
      "i=0\n",
      "logging_setup\n",
      "training cost fn   0.054330\n",
      "val cost fn        0.054306\n",
      "val log likelihood fn        1318.818362\n",
      "\n",
      "\n",
      "i=1\n",
      "logging_setup\n",
      "training cost fn   0.054330\n",
      "val cost fn        0.054306\n",
      "val log likelihood fn        1318.818362\n",
      "\n",
      "\n",
      "i=2\n",
      "logging_setup\n",
      "training cost fn   0.054330\n",
      "val cost fn        0.054306\n",
      "val log likelihood fn        1318.818362\n",
      "\n",
      "\n",
      "[1318.81836243 1318.81836243 1318.81836243]\n",
      "logging_setup\n"
     ]
    }
   ],
   "source": [
    "#kernels_grid = [kernels.sklearn_best()]\n",
    "kernels_grid = [\n",
    "    RBF(),\n",
    "    1.0*RBF()+WhiteKernel(),\n",
    "    1.0*Matern(nu=1.5)+WhiteKernel(),\n",
    "    1.0*Matern(nu=2.5)+WhiteKernel(),\n",
    "    1.0+1.0*Matern(nu=1.5)+WhiteKernel(),\n",
    "    1.0+1.0*Matern(nu=2.5)+WhiteKernel()\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "kernels_grid = [\n",
    "    1.0*RBF()+WhiteKernel(),\n",
    "    0.5*RBF()+0.5*RationalQuadratic()+WhiteKernel(),\n",
    "    0.33*RBF()+0.33*RationalQuadratic()+0.33*DotProduct()+WhiteKernel(),\n",
    "    1.0*Matern(nu=1.5)+WhiteKernel(),\n",
    "    1.0+1.0*Matern(nu=1.5)+WhiteKernel()\n",
    "]\n",
    "\"\"\"\n",
    "K_cv = 3\n",
    "model_config = {\n",
    "    \"use_skit_learn\":True,\n",
    "    \"use_nystrom\":False,\n",
    "    \"use_fitc\" : False,\n",
    "    \"model_preprocess_left_frac\":1.0\n",
    "}\n",
    "\n",
    "result_csv_path = \"cv_results3.csv\"\n",
    "tune_HP_nested_CV2(kernels_grid, K_cv, model_config,result_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernels_grid = [kernels.sklearn_best()]\n",
    "kernels_tunable = 1.0*RBF()+WhiteKernel()+ 1.0*Matern()\n",
    "\n",
    "K_cv = 3\n",
    "model_config = {\n",
    "    \"use_skit_learn\":True,\n",
    "    \"use_nystrom\":False,\n",
    "    \"use_fitc\" : False,\n",
    "    \"model_preprocess_left_frac\":1.0\n",
    "}\n",
    "\n",
    "result_csv_path = \"/Users/laurieprelot/Documents/Projects/2020_probabilistic_AI/task1_handout/data/LML_results.csv\"\n",
    "tune_HP_nested_LML(kernels_tunable, K_cv, model_config,result_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### TODO add a simple outer train, test split to select the hyperparameters (kernel type etc) in the inner CV loop \n",
    "# kernel = kernels.sklearn_best()\n",
    "# K_cv = 10\n",
    "# model_config = {\n",
    "#     \"kernel\":kernel,\n",
    "#     \"use_skit_learn\":True,\n",
    "#     \"model_preprocess_left_frac\":1.0\n",
    "# }\n",
    "\n",
    "# cv_m = cv_eval(cv_splits = K_cv,\n",
    "#                cv_preprocess_left_frac = 0.05,\n",
    "#                model_config=model_config) # Choose whether to use custom model or scikit learn\n",
    "# val_cost_array = cv_m.run_cross_validation(train_x, train_y)\n",
    "\n",
    "# print(np.mean(val_cost_array))\n",
    "# print(np.std(val_cost_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call FITC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernels_grid = [kernels.sklearn_best()]\n",
    "K_cv = 10\n",
    "model_config = {\n",
    "    \"use_skit_learn\":False,\n",
    "    \"use_nystrom\":False,\n",
    "    \"use_fitc\":True,\n",
    "    \"model_preprocess_left_frac\":1.0\n",
    "}\n",
    "tune_HP_nested_CV(kernels_grid, K_cv, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TODO add a simple outer train, test split to select the hyperparameters (kernel type etc) in the inner CV loop \n",
    "# kernel = kernels.sklearn_best()\n",
    "# K_cv = 10\n",
    "# model_config = {\n",
    "#     \"kernel\":kernel,\n",
    "#     \"use_skit_learn\":False,\n",
    "#     \"use_nystrom\":False,\n",
    "#     \"use_fitc\":True,\n",
    "#     \"model_preprocess_left_frac\":1.0\n",
    "# }\n",
    "# cv_m = cv_eval(cv_splits = K_cv,\n",
    "#                cv_preprocess_left_frac = 0.05,\n",
    "#                model_config=model_config) # Choose whether to use custom model or scikit learn\n",
    "# val_cost_array = cv_m.run_cross_validation(train_x, train_y)\n",
    "\n",
    "# print(np.mean(val_cost_array))\n",
    "# print(np.std(val_cost_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_m.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Nystrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_grid = [kernels.sklearn_best()]\n",
    "K_cv = 10\n",
    "model_config = {\n",
    "    \"use_skit_learn\":False,\n",
    "    \"use_nystrom\":True,\n",
    "    \"use_fitc\":False,\n",
    "    \"model_preprocess_left_frac\":1.0\n",
    "}\n",
    "tune_HP_nested_CV(kernels_grid, K_cv, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TODO add a simple outer train, test split to select the hyperparameters (kernel type etc) in the inner CV loop \n",
    "# kernel = kernels.sklearn_best()\n",
    "# K_cv = 10\n",
    "# model_config = {\n",
    "#     \"kernel\":kernel,\n",
    "#     \"use_skit_learn\":False,\n",
    "#     \"use_nystrom\":True,\n",
    "#     \"use_fitc\":False,\n",
    "#     \"model_preprocess_left_frac\":1.0\n",
    "# }\n",
    "# cv_m = cv_eval(cv_splits = K_cv,\n",
    "#                cv_preprocess_left_frac = 0.05,\n",
    "#                model_config=model_config) # Choose whether to use custom model or scikit learn\n",
    "# val_cost_array = cv_m.run_cross_validation(train_x, train_y)\n",
    "\n",
    "# print(np.mean(val_cost_array))\n",
    "# print(np.std(val_cost_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create New CV Results csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normally no need to call\n",
    "\n",
    "cv_result_cols = [\"test_cost\",\"val_cost\",\"lml\",\"init_kernel\",\"left_data\",\"cv_config\",\"model_config\",\"time\"]\n",
    "df_cv_result = pd.DataFrame(columns = cv_result_cols)\n",
    "#df_cv_result\n",
    "result_csv_path = \"cv_results3.csv\"\n",
    "df_cv_result.to_csv(result_csv_path,index=False,mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
